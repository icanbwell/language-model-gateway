import abc
from abc import abstractmethod
from typing import AsyncIterable, Literal, Any, List, Optional

from langchain_core.messages import AnyMessage
from langchain_core.messages.ai import UsageMetadata
from starlette.responses import StreamingResponse, JSONResponse

from language_model_gateway.configs.config_schema import AgentConfig
from language_model_gateway.gateway.structures.openai.message.chat_message_wrapper import (
    ChatMessageWrapper,
)


class ChatRequestWrapper(abc.ABC):
    """This is the abstract base class for ChatCompletionRequestWrapper and ResponsesApiRequestWrapper."""

    """model: The model to use for the chat request.  This is used for tools that need to know which model is being used, such as a tool that extracts a JSON object from the message content and needs to know which model's response format to expect."""

    @property
    @abstractmethod
    def model(self) -> str: ...

    """messages: The messages in the chat request.  This is used for tools that need to read the messages in the chat request, such as a tool that extracts a JSON object from the message content."""

    @property
    @abstractmethod
    def messages(self) -> list[ChatMessageWrapper]: ...

    @messages.setter
    @abstractmethod
    def messages(self, value: list[ChatMessageWrapper]) -> None: ...

    """ Append a message to the messages in the chat request.  This is used for tools that need to add a message to the end of the messages, such as a tool that adds a system message to the end of the messages."""

    @abstractmethod
    def append_message(self, *, message: ChatMessageWrapper) -> None: ...

    """ Create a system message and append it to the messages in the chat request.  This is used for tools that need to add a system message to the messages, such as a tool that adds a system message to the beginning of the messages."""

    @abstractmethod
    def create_system_message(self, *, content: str) -> ChatMessageWrapper: ...

    """stream: Whether the response should be streamed back in chunks as they are generated, or returned all at once after the model has finished generating.  This is used for tools that need to know whether the response will be streamed or not, such as a tool that extracts a JSON object from the message content and needs to know whether to expect multiple messages with partial content or just one message with the full content."""

    @property
    @abstractmethod
    def stream(self) -> Literal[False, True] | None | bool: ...

    """response_format: The format that the response should be in.  This is used for tools that need to know the expected format of the response, such as a tool that extracts a JSON object from the message content and needs to know whether to expect plain text, a JSON object, or a JSON schema."""

    @property
    @abstractmethod
    def response_format(
        self,
    ) -> Literal["text", "json_object", "json_schema"] | None: ...

    """response_json_schema: If response_format is "json_schema", this is the JSON schema that the response should conform to.  This is used for tools that need to validate the format of the response, such as a tool that extracts a JSON object from the message content and needs to validate that the JSON object conforms to the expected schema."""

    @property
    @abstractmethod
    def response_json_schema(self) -> str | None: ...

    """ Create the first message to be sent in the streaming response, which typically contains metadata about the response such as the request ID.  This is used for tools that need to create the initial message in a streaming response, such as a tool that creates a message with the request ID that will be sent before any content messages are sent."""

    @abstractmethod
    def create_first_sse_message(self, *, request_id: str) -> str: ...

    """ Create a message to be sent in the streaming response for a chunk of content generated by the model.  This is used for tools that need to create messages for each chunk of content in a streaming response, such as a tool that creates messages with usage metadata for each chunk of content."""

    @abstractmethod
    def create_sse_message(
        self,
        *,
        request_id: str,
        content: str | None,
        usage_metadata: UsageMetadata | None,
    ) -> str: ...

    @abstractmethod
    def create_debug_sse_message(
        self,
        *,
        request_id: str,
        content: str | None,
        usage_metadata: UsageMetadata | None,
    ) -> str: ...

    """ Create the final message to be sent in the streaming response after the model has finished generating, which typically contains metadata about the response such as usage information.  This is used for tools that need to create the final message in a streaming response, such as a tool that creates a message with usage metadata that will be sent after all content messages have been sent."""

    @abstractmethod
    def create_final_sse_message(
        self,
        *,
        request_id: str,
        usage_metadata: UsageMetadata | None,
    ) -> str: ...

    """ Create the response to be sent back when not streaming, which typically contains the full content generated by the model and any relevant metadata.  This is used for tools that need to create the response for a non-streaming request, such as a tool that creates a JSON object with the full content and usage metadata to be sent back in the response."""

    @abstractmethod
    def create_non_streaming_response(
        self,
        *,
        request_id: str,
        json_output_requested: Optional[bool],
        responses: List[AnyMessage],
    ) -> dict[str, Any]: ...

    """ Convert the chat request wrapper to a dictionary format that can be used for logging or other purposes.  This is used for tools that need to convert the chat request to a dictionary format, such as a tool that logs the chat request in a structured format."""

    @abstractmethod
    def to_dict(self) -> dict[str, Any]: ...

    """get_tools: Get a list of tools that should be used for this chat request.  This is used for tools that need to know which tools to use for a given chat request, such as a tool that extracts a JSON object from the message content and needs to know which tool to use to extract the JSON object."""

    @abstractmethod
    def get_tools(self) -> list[AgentConfig]: ...

    """stream_response: Stream the response back in chunks as they are generated.  This is used for tools that need to stream the response back, such as a tool that extracts a JSON object from the message content and needs to receive the content in chunks as it is generated."""

    @abstractmethod
    def stream_response(
        self,
        *,
        response_messages1: List[AnyMessage],
    ) -> AsyncIterable[str]: ...

    def write_response(
        self,
        *,
        request_id: str,
        response_messages: List[AnyMessage],
    ) -> StreamingResponse | JSONResponse:
        """Write the response back to the client, either as a streaming response or a non-streaming response depending on the value of self.stream.  This is used for tools that need to write the response back to the client, such as a tool that writes the response in chunks as it is generated or writes the full response after it has been generated."""
        should_stream_response: Optional[bool] = self.stream

        if should_stream_response:
            stream_content: AsyncIterable[str] = self.stream_response(
                response_messages1=response_messages
            )
            return StreamingResponse(
                content=stream_content,
                media_type="text/event-stream",
            )
        else:
            chat_response = self.create_non_streaming_response(
                request_id=request_id,
                responses=response_messages,
                json_output_requested=False,
            )
            return JSONResponse(content=chat_response)

    def write_non_streaming_response(
        self, *, request_id: str, response_messages: list[AnyMessage]
    ) -> JSONResponse:
        """Write a non-streaming response back to the client, which typically contains the full content generated by the model and any relevant metadata.  This is used for tools that need to write a non-streaming response back to the client, such as a tool that creates a JSON object with the full content and usage metadata to be sent back in the response."""
        chat_response = self.create_non_streaming_response(
            request_id=request_id,
            responses=response_messages,
            json_output_requested=False,
        )
        return JSONResponse(content=chat_response)

    """ The following properties are used for tools that need to read or modify the chat request, such as a tool that extracts a JSON object from the message content and needs to know the instructions for how to extract the JSON object, or a tool that adds a system message with instructions to the messages in the chat request."""

    """ instructions: Instructions for how the model should generate the response.  This is used for tools that need to know the instructions for generating the response, such as a tool that extracts a JSON object from the message content and needs to know the instructions for how to extract the JSON object."""

    @property
    @abstractmethod
    def instructions(self) -> Optional[str]: ...

    """ previous_response_id: The ID of the previous response in the conversation, if this chat request is part of an ongoing conversation.  This is used for tools that need to know the context of the conversation, such as a tool that extracts a JSON object from the message content and needs to know the previous response to understand the context for how to extract the JSON object."""

    @property
    @abstractmethod
    def previous_response_id(self) -> Optional[str]: ...

    """ store: Whether the chat request and its response should be stored in the database.  This is used for tools that need to know whether to store the chat request and response, such as a tool that extracts a JSON object from the message content and needs to know whether to store the extracted JSON object in the database."""

    @property
    @abstractmethod
    def store(self) -> Optional[bool]: ...

    """ user_input: The original user input that led to this chat request, if available.  This is used for tools that need to know the original user input, such as a tool that extracts a JSON object from the message content and needs to know the original user input to understand the context for how to extract the JSON object."""

    @property
    @abstractmethod
    def user_input(self) -> Optional[str]: ...

    """ metadata: Any additional metadata associated with the chat request.  This is used for tools that need to know any additional metadata associated with the chat request, such as a tool that extracts a JSON object from the message content and needs to know any additional metadata to understand the context for how to extract the JSON object."""

    @property
    @abstractmethod
    def metadata(self) -> Optional[dict[str, Any]]: ...

    """ max_tokens: The maximum number of tokens that the model should generate in the response.  This is used for tools that need to know the maximum number of tokens for the response, such as a tool that extracts a JSON object from the message content and needs to know how much content to expect in the response."""

    @property
    @abstractmethod
    def max_tokens(self) -> Optional[int]: ...

    """ max_output_tokens: The maximum number of tokens that the model should generate in the response, including any tokens used for formatting or other purposes.  This is used for tools that need to know the maximum number of tokens for the response, such as a tool that extracts a JSON object from the message content and needs to know how much content to expect in the response including any formatting tokens."""

    @property
    @abstractmethod
    def max_output_tokens(self) -> Optional[int]: ...

    """ temperature: The temperature to use for the response generation, which controls the randomness of the generated response.  This is used for tools that need to know the temperature for the response generation, such as a tool that extracts a JSON object from the message content and needs to know how much variability to expect in the response."""

    @property
    @abstractmethod
    def temperature(self) -> Optional[float]: ...

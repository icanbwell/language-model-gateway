{
    "$schema": "https://raw.githubusercontent.com/imranq2/language_model_gateway/main/language_model_gateway/configs/config_schema.json",
    "id": "bug_resolution_prompt",
    "name": "Bug Resolution Prompt",
    "description": "An AI assistant to finds possible solutions for a bug, taking error stacktrace as input and an aim to provide high accuracy.",
    "owner": "Sumit Lohan, Deep Jha, RitujAryan",
    "type": "langchain",
    "model": {
        "provider": "bedrock",
        "model": "us.anthropic.claude-3-5-haiku-20241022-v1:0"
    },
    "system_prompts": [
        {
            "role": "system",
            "content": "You are an advanced stack trace analysis assistant with the following strict guidelines:\n\n1. ALWAYS prioritize accuracy and precision in error resolution\n2. Focus EXCLUSIVELY on the provided stack trace as your context\n3. Resolution process:\n   a) Identify the primary error line\n   b) Check internal bugs_resolutions.csv for exact match\n   c) If no exact match, conduct web research for potential solutions\n4. Provide clear, concise resolution steps\n5. If no definitive solution is found, clearly state limitations\n6. Never fabricate or speculate beyond available evidence\n\nResolution Format:\n- Primary Error Identified: [Specific Error Line]\n- Source of Solution: [Internal CSV / Web Search]\n- Recommended Action: [Precise Steps]\n- Confidence Level: [High/Medium/Low]"
        }
    ],
    "tools": [
        {
            "name": "bug_identifier"
        },
        {
            "name": "web_search"
        },
        {
            "name": "scraping_bee_web_scraper"
        }
    ],
    "example_prompts": [
        {
            "role": "user",
            "content": "at org.apache.spark.scheduler.Task.run(Task.scala:141)\n    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n    at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n    at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n    ... 1 more\nCaused by: java.sql.SQLIntegrityConstraintViolationException: Duplicate entry 's3://bwell-ingestion/preprocessed/thedacare_internal_employee/Th' for key 'PRIMARY'\n    at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:118)\n    at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\n    at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:916)\n    at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1061)\n    at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1009)\n    at com.mysql.cj.jdbc.ClientPreparedStatement.executeLargeUpdate(ClientPreparedStatement.java:1320)\n    at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchWithMultiValuesClause(ClientPreparedStatement.java:679)\n    ... 19 more"
        },
        {
            "role": "user",
            "content": "java.lang.OutOfMemoryError: Java heap space\n    at org.apache.spark.memory.MemoryManager.allocateMemory(MemoryManager.scala:324)\n    at org.apache.spark.storage.BlockManager.doGetBlockData(BlockManager.scala:456)\n    at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:402)\n    at org.apache.spark.storage.BlockManager.getLocalBytes(BlockManager.scala:275)\n    at org.apache.spark.storage.BlockManager.getBytes(BlockManager.scala:234)\n    at org.apache.spark.storage.BlockManager.get(BlockManager.scala:210)\n    at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:345)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n    ... more"
        }
    ],
    "model_parameters": [
        {
            "key": "max_tokens",
            "value": "1000"
        },
        {
            "key": "temperature",
            "value": "0.3"
        }
    ]
}
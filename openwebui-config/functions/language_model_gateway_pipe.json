[
  {
    "id": "language_model_gateway",
    "user_id": "6c03bf27-dbbf-44c1-b980-2c6a4608f712",
    "name": "language_model_gateway",
    "type": "pipe",
    "content": "\"\"\"title: LangChain Pipe Function (Streaming Version)\nauthor: Imran Qureshi @ b.well Connected Health (mailto:imran.qureshi@bwell.com)\nauthor_url: https://github.com/imranq2\nversion: 0.2.0\nThis module defines a Pipe class that reads the oauth_id_token from the request cookies and uses it in Authorization header\nto make requests to the OpenAI API. It supports both streaming and non-streaming responses.\n\"\"\"\n\nimport datetime\nimport json\nimport logging\nimport os\nimport time\nfrom pathlib import PurePosixPath\nfrom typing import (\n    AsyncGenerator,\n    List,\n    Optional,\n    Callable,\n    Awaitable,\n    Any,\n    Dict,\n    Generator,\n)\nfrom urllib.parse import urlparse, urlunparse\n\nimport httpx\nfrom pydantic import BaseModel, Field\nfrom starlette.requests import Request\n\nlogger = logging.getLogger(__name__)\n\n# Cache TTL in seconds (60 minutes)\nCACHE_TTL_SECONDS = 60 * 60\n\nLLM_CALL_TIMEOUT = 60 * 5  # 5 minutes\n\n\nclass Pipe:\n    \"\"\"\n    Pipe class for interacting with the OpenAI API using OAuth ID token from request cookies.\n    Supports both streaming and non-streaming responses.\n    \"\"\"\n\n    class Valves(BaseModel):\n        emit_interval: float = Field(\n            default=2.0, description=\"Interval in seconds between status emissions\"\n        )\n        enable_status_indicator: bool = Field(\n            default=True, description=\"Enable or disable status indicator emissions\"\n        )\n        OPENAI_API_BASE_URL: Optional[str] = Field(\n            default=None,\n            description=\"Base URL for OpenAI API, e.g., https://api.openai.com/v1\",\n        )\n        model_name_prefix: Optional[str] = Field(\n            default=None, description=\"Prefix for model names in the dropdown\"\n        )\n        restrict_to_admins: bool = Field(\n            default=False,\n            description=\"Restrict access to this pipe to admin users only\",\n        )\n        restrict_to_model_ids: List[str] = Field(\n            default_factory=list,\n            description=\"List of model IDs to restrict access to. If empty, no restriction is applied.\",\n        )\n        debug_mode: bool = Field(\n            default=False,\n            description=\"Enable debug mode for additional logging and debugging information\",\n        )\n        default_model: Optional[str] = Field(\n            default=\"General Purpose\", description=\"Default model to use\"\n        )\n\n    def __init__(self) -> None:\n        self.type: str = \"pipe\"\n        self.id: str = \"language_model_gateway\"\n        openai_api_base_url_ = self.read_base_url()\n        self.valves = self.Valves(OPENAI_API_BASE_URL=openai_api_base_url_)\n        self.name: str = (\n            self.valves.model_name_prefix.strip()\n            if self.valves.model_name_prefix\n            else \"\"\n        )\n        self.last_emit_time: float = 0\n        self.pipelines: Optional[List[Dict[str, Any]]] = None\n        self.pipelines_last_updated: Optional[float] = (\n            None  # Track last cache update time\n        )\n        self.default_model: Optional[str] = (\n            os.getenv(\"DEFAULT_MODELS\") or self.valves.default_model\n        )\n\n    @staticmethod\n    def read_base_url() -> Optional[str]:\n        \"\"\"Reads the OpenAI API base URL from environment variables.\"\"\"\n        return os.getenv(\"LANGUAGE_MODEL_GATEWAY_API_BASE_URL\") or os.getenv(\n            \"OPENAI_API_BASE_URL\"\n        )\n\n    async def on_startup(self) -> None:\n        logger.debug(f\"on_startup:{__name__}\")\n        self.pipelines = await self.get_models()\n\n    # noinspection PyMethodMayBeStatic\n    async def on_shutdown(self) -> None:\n        logger.debug(f\"on_shutdown:{__name__}\")\n\n    async def on_valves_updated(self) -> None:\n        logger.debug(f\"on_valves_updated:{__name__}\")\n        self.pipelines = await self.get_models()\n\n    async def emit_status(\n        self,\n        __event_emitter__: Optional[Callable[[Dict[str, Any]], Awaitable[None]]],\n        level: str,\n        message: str,\n        done: bool,\n        message_type: str = \"status\",\n    ) -> None:\n        \"\"\"Emit status updates at controlled intervals.\"\"\"\n        current_time = time.time()\n        if (\n            __event_emitter__\n            and self.valves.enable_status_indicator\n            and (\n                current_time - self.last_emit_time >= self.valves.emit_interval or done\n            )\n        ):\n            await __event_emitter__(\n                {\n                    \"type\": message_type,\n                    \"data\": {\n                        \"status\": \"complete\" if done else \"in_progress\",\n                        \"level\": level,\n                        \"description\": message,\n                        \"done\": done,\n                    },\n                }\n            )\n            self.last_emit_time = current_time\n\n    @classmethod\n    def pathlib_url_join(cls, base_url: str, path: str) -> str:\n        \"\"\"Join URLs using pathlib for path manipulation.\"\"\"\n        parsed_base = urlparse(base_url)\n        full_path = str(PurePosixPath(parsed_base.path) / path.lstrip(\"/\"))\n        reconstructed_url = urlunparse(\n            (\n                parsed_base.scheme,\n                parsed_base.netloc,\n                full_path,\n                parsed_base.params,\n                parsed_base.query,\n                parsed_base.fragment,\n            )\n        )\n        return reconstructed_url\n\n    @staticmethod\n    def log_httpx_request(request: httpx.Request) -> str:\n        \"\"\"Convert an HTTPX request to a detailed string representation.\"\"\"\n        request_log = f\"\"\"\nHTTPX Request:\n- Method: {request.method}\n- URL: {request.url}\n- Headers: {dict(request.headers)}\n- Body: {request.content.decode(\"utf-8\", errors=\"replace\") if request.content else \"No body\"}\n\"\"\".strip()\n        return request_log\n\n    @staticmethod\n    def log_response_as_string(response1: httpx.Response) -> str:\n        \"\"\"Convert an HTTPX response to a detailed, formatted string.\"\"\"\n        try:\n            try:\n                response_body = json.dumps(response1.json(), indent=2)\n            except (ValueError, json.JSONDecodeError):\n                response_body = response1.text[:1000]\n        except Exception:\n            response_body = \"(Unable to decode response body)\"\n        response_log = f\"\"\"\nHTTPX Response Log:\n- Timestamp: {datetime.datetime.now().isoformat()}\n- Status Code: {response1.status_code}\n- URL: {response1.request.url}\n- Method: {response1.request.method}\n- Response Headers:\n{json.dumps(dict(response1.headers), indent=2)}\n- Response Body:\n{response_body}\n- Response Encoding: {response1.encoding}\n- Response Elapsed Time: {response1.elapsed}\n\"\"\".strip()\n        return response_log\n\n    @staticmethod\n    def _build_headers(\n        *,\n        request: Request,\n        user: Optional[Dict[str, Any]],\n        access_token: Optional[str],\n        id_token: Optional[str],\n        session_id: Optional[str],\n        chat_id: Optional[str],\n        message_id: Optional[str],\n    ) -> Dict[str, str]:\n        \"\"\"\n        Build headers for the OpenAI API request, including user and request context.\n\n        \"\"\"\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"X-ID-Token\": id_token or \"\",\n            \"X-Session-Id\": session_id or \"\",\n            \"X-Chat-Id\": chat_id or \"\",\n            \"X-Message-Id\": message_id or \"\",\n        }\n        for key in [\n            \"User-Agent\",\n            \"Referrer\",\n            \"Cookie\",\n            \"traceparent\",\n            \"origin\",\n            \"Accept-Encoding\",\n        ]:\n            if key in request.headers:\n                headers[key] = request.headers[key]\n        if user:\n            for user_key, header_key in [\n                (\"name\", \"X-OpenWebUI-User-Name\"),\n                (\"id\", \"X-OpenWebUI-User-Id\"),\n                (\"email\", \"X-OpenWebUI-User-Email\"),\n                (\"role\", \"X-OpenWebUI-User-Role\"),\n            ]:\n                if user.get(user_key):\n                    headers[header_key] = user[user_key]\n            info = user.get(\"info\")\n            if info and isinstance(info, dict) and info.get(\"location\"):\n                headers[\"X-OpenWebUI-User-Location\"] = info[\"location\"]\n        for key, value in request.headers.items():\n            if key.lower().startswith(\"x-\"):\n                headers[key] = value\n        return headers\n\n    def _yield_debug_info(\n        self,\n        *,\n        user: Optional[Dict[str, Any]],\n        request: Request,\n        url: str,\n        headers: Dict[str, str],\n        payload: Dict[str, Any],\n    ) -> Generator[str, None, None]:\n        if self.valves.debug_mode:\n            yield f\"User:\\n{json.dumps(user, indent=2) if user else None}\\n\"\n            yield f\"Original Headers:\\n{dict(request.headers)}\\n\"\n            yield url + \"\\n\"\n            yield f\"New Headers: {dict(headers)}\\n\"\n            yield json.dumps(payload) + \"\\n\"\n            info = user.get(\"info\") if user else None\n            if info and isinstance(info, dict) and info.get(\"location\"):\n                yield f\"Location: {type(info['location']).__name__} {info['location']}\\n\"\n\n    @staticmethod\n    def _make_error_chunk(\n        *, error: Exception, is_streaming: bool\n    ) -> Optional[Dict[str, Any]]:\n        if is_streaming:\n            return {\n                \"id\": \"chatcmpl-error\",\n                \"object\": \"chat.completion.chunk\",\n                \"created\": int(time.time()),\n                \"model\": \"error\",\n                \"choices\": [\n                    {\n                        \"index\": 0,\n                        \"delta\": {\"content\": f\"Error [{type(error)}]: {error}\"},\n                        \"finish_reason\": \"stop\",\n                    }\n                ],\n            }\n        return None\n\n    async def pipe(\n        self,\n        body: Dict[str, Any],\n        __request__: Optional[Request] = None,\n        __user__: Optional[Dict[str, Any]] = None,\n        __event_emitter__: Optional[Callable[[Dict[str, Any]], Awaitable[None]]] = None,\n        __event_call__: Optional[\n            Callable[[Dict[str, Any]], Awaitable[Dict[str, Any]]]\n        ] = None,\n        __oauth_token__: Optional[Dict[str, Any]] = None,\n        __chat_id__: Optional[str] = None,\n        __session_id__: Optional[str] = None,\n        __message_id__: Optional[str] = None,\n        __metadata__: Optional[Dict[str, Any]] = None,\n        __files__: Optional[List[str]] = None,\n    ) -> AsyncGenerator[str, None]:\n        \"\"\"\n        Main pipe method supporting both streaming and non-streaming responses.\n        \"\"\"\n        if not __oauth_token__ or \"access_token\" not in __oauth_token__:\n            yield \"Oops, looks like your Auth token has expired. Please logout and login to Aiden to get a new Auth token.\"\n            return\n        access_token: Optional[str] = __oauth_token__.get(\"access_token\")\n        id_token: Optional[str] = __oauth_token__.get(\"id_token\")\n        await self.emit_status(__event_emitter__, \"info\", \"Working...\", False)\n        logger.debug(f\"pipe:{__name__}\")\n        logger.debug(f\"body: {body}\")\n        logger.debug(f\"__request__: {__request__}\")\n        logger.debug(f\"__user__: {__user__}\")\n        logger.debug(f\"Request URL: {getattr(__request__, 'url', None)}\")\n        if __request__ is None:\n            raise ValueError(\"Request object must be provided.\")\n        auth_header = __request__.headers.get(\"Authorization\")\n        logger.debug(f\"Authorization header: {auth_header if auth_header else 'None'}\")\n        open_api_base_url: Optional[str] = (\n            self.valves.OPENAI_API_BASE_URL or self.read_base_url()\n        )\n        if not open_api_base_url:\n            raise RuntimeError(\n                \"OpenAI API base URL must be set as an environment variable.\"\n            )\n        logger.debug(f\"open_api_base_url: {open_api_base_url}\")\n        model_id = body.get(\"model\", \"\")\n        if \".\" in model_id:\n            model_id = model_id.split(\".\", 1)[1]\n        payload = {**body, \"model\": model_id}\n        url = self.pathlib_url_join(base_url=open_api_base_url, path=\"chat/completions\")\n        response_text: str = \"\"\n        is_streaming: bool = body.get(\"stream\", False)\n        headers = self._build_headers(\n            request=__request__,\n            user=__user__,\n            access_token=access_token,\n            id_token=id_token,\n            session_id=__session_id__,\n            chat_id=__chat_id__,\n            message_id=__message_id__,\n        )\n        for debug_line in self._yield_debug_info(\n            user=__user__,\n            request=__request__,\n            url=url,\n            headers=headers,\n            payload=payload,\n        ):\n            yield debug_line\n        try:\n            logger.debug(\n                f\"Calling chat completion url: {url} with payload: {payload} and headers: {__request__.headers}\"\n            )\n            async with httpx.AsyncClient() as client:\n                response = await client.post(\n                    url=url,\n                    json=payload,\n                    headers=headers,\n                    timeout=LLM_CALL_TIMEOUT,\n                    follow_redirects=True,\n                )\n                response.raise_for_status()\n                content_type = response.headers.get(\"content-type\", \"\")\n                if content_type.startswith(\"text/event-stream\"):\n                    async for line in response.aiter_lines():\n                        if line:\n                            yield line + \"\\n\"\n                else:\n                    yield json.dumps(response.json())\n            await self.emit_status(__event_emitter__, \"info\", \"Done\", True)\n        except httpx.HTTPStatusError as e:\n            await self.emit_status(__event_emitter__, \"HttpError\", f\"{e}\", True)\n            yield (\n                f\"LanguageModelGateway::pipe HTTP Status Error: {type(e)} {e}\\n\"\n                + f\"{self.log_httpx_request(e.request)}\\n\"\n                + f\"{self.log_response_as_string(e.response)}\"\n            )\n        except Exception as e:\n            await self.emit_status(__event_emitter__, \"error\", f\"{e}\", True)\n            httpx_version = getattr(httpx, \"__version__\", \"unknown\")\n            error_chunk = self._make_error_chunk(error=e, is_streaming=is_streaming)\n            if error_chunk:\n                yield f\"data: {json.dumps(error_chunk)}\\n\\n\"\n            else:\n                yield (\n                    f\"LanguageModelGateway::pipe Error:\"\n                    f\" {type(e)} {e} httpx_version={httpx_version} url={url}\"\n                    f\" original_url={getattr(__request__, 'url', None)}\"\n                    f\" response_text={response_text} payload={payload}\\n\"\n                )\n\n    async def get_models(self) -> List[Dict[str, str]]:\n        \"\"\"Fetches the list of available models from the OpenAI API.\"\"\"\n        open_api_base_url: Optional[str] = (\n            self.valves.OPENAI_API_BASE_URL or self.read_base_url()\n        )\n        if not open_api_base_url:\n            logger.debug(\"OpenAI API base URL is not set.\")\n            return []\n        model_url = self.pathlib_url_join(base_url=open_api_base_url, path=\"models\")\n        logger.debug(f\"Calling models endpoint: {model_url}\")\n        async with httpx.AsyncClient() as client:\n            response = await client.get(url=model_url, timeout=30.0)\n            response.raise_for_status()\n            models = response.json().get(\"data\", [])\n        logger.debug(f\"Received models from {model_url}: {models}\")\n        # Update cache timestamp\n        self.pipelines_last_updated = time.time()\n        return [{\"id\": model[\"id\"], \"name\": model[\"id\"]} for model in models]\n\n    async def pipes(self) -> List[Dict[str, str]]:\n        now = time.time()\n        cache_expired = (\n            self.pipelines is None\n            or self.pipelines_last_updated is None\n            or (now - self.pipelines_last_updated) > CACHE_TTL_SECONDS\n        )\n        if cache_expired:\n            logger.debug(\"Model cache expired or not set. Fetching models.\")\n            self.pipelines = await self.get_models()\n\n        models = self.pipelines or []\n        if self.valves.restrict_to_model_ids:\n            models = [\n                model\n                for model in models\n                if model[\"id\"] in self.valves.restrict_to_model_ids\n            ]\n\n        # Always put default_model at the top\n        default_model_id = self.valves.default_model\n        if default_model_id:\n            # Remove any existing entry for default_model\n            models = [m for m in models if m[\"id\"] != default_model_id]\n            # Insert default_model at the top\n            models.insert(0, {\"id\": default_model_id, \"name\": default_model_id})\n\n        return models\n",
    "meta": {
      "description": "Talks to Language Model Gateway and passes the OAuth ID token in the request cookies as Bearer Authorization header.",
      "manifest": {
        "title": "Language Model Gateway Pipe",
        "author": "Imran Qureshi @ b.well Connected Health (mailto:imran.qureshi@bwell.com)",
        "author_url": "https://github.com/imranq2",
        "version": "0.1.0"
      }
    },
    "is_active": true,
    "is_global": true,
    "updated_at": 1745989387,
    "created_at": 1745989309
  }
]
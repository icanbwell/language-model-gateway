[
  {
    "id": "language_model_gateway",
    "user_id": "6c03bf27-dbbf-44c1-b980-2c6a4608f712",
    "name": "language_model_gateway",
    "type": "pipe",
    "content": "\"\"\"\ntitle: LangChain Pipe Function\nauthor: Colby Sawyer @ Attollo LLC (mailto:colby.sawyer@attollodefense.com)\nauthor_url: https://github.com/ColbySawyer7\nversion: 0.1.0\n\nThis module defines a Pipe class that utilizes LangChain\n\"\"\"\n\nimport time\nfrom typing import Optional, Callable, Awaitable, Any, Dict\n\nfrom pydantic import BaseModel, Field\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        emit_interval: float = Field(\n            default=2.0, description=\"Interval in seconds between status emissions\"\n        )\n        enable_status_indicator: bool = Field(\n            default=True, description=\"Enable or disable status indicator emissions\"\n        )\n\n    def __init__(self) -> None:\n        self.type: str = \"pipe\"\n        self.id: str = \"langchain_pipe\"\n        self.name: str = \"LangChain Pipe\"\n        self.valves = self.Valves()\n        self.last_emit_time: float = 0\n        pass\n\n    async def emit_status(\n        self,\n        __event_emitter__: Callable[[Dict[str, Any]], Awaitable[None]] | None,\n        level: str,\n        message: str,\n        done: bool,\n    ) -> None:\n        current_time = time.time()\n        if (\n            __event_emitter__\n            and self.valves.enable_status_indicator\n            and (\n                current_time - self.last_emit_time >= self.valves.emit_interval or done\n            )\n        ):\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": \"complete\" if done else \"in_progress\",\n                        \"level\": level,\n                        \"description\": message,\n                        \"done\": done,\n                    },\n                }\n            )\n            self.last_emit_time = current_time\n\n    async def pipe(\n        self,\n        body: Dict[str, Any],\n        __request__: Optional[Dict[str, Any]] = None,\n        __user__: Optional[Dict[str, Any]] = None,\n        __event_emitter__: Callable[[Dict[str, Any]], Awaitable[None]] | None = None,\n        __event_call__: Callable[[Dict[str, Any]], Awaitable[Dict[str, Any]]]\n        | None = None,\n    ) -> Optional[Dict[str, Any]]:\n        try:\n            await self.emit_status(\n                __event_emitter__,\n                \"info\",\n                f\"/initiating Chain: {__request__=} {__user__=} {body=}\",\n                False,\n            )\n            # user = None\n            # await self.emit_status(\n            #     __event_emitter__, \"info\", \"Starting Chain\", False\n            # )\n            # result = cast(Dict[str,Any], await generate_chat_completion(__request__, body, user))\n            result: Dict[str, Any] = {\n                \"id\": \"chatcmpl-B9MBs8CjcvOU2jLn4n570S5qMJKcT\",\n                \"object\": \"chat.completion\",\n                \"created\": 1741569952,\n                \"model\": \"gpt-4.1-2025-04-14\",\n                \"choices\": [\n                    {\n                        \"index\": 0,\n                        \"message\": {\n                            \"role\": \"assistant\",\n                            \"content\": \"Hello! How can I assist you today?\",\n                            \"annotations\": [],\n                        },\n                        \"logprobs\": None,\n                        \"finish_reason\": \"stop\",\n                    }\n                ],\n                \"usage\": {\n                    \"prompt_tokens\": 19,\n                    \"completion_tokens\": 10,\n                    \"total_tokens\": 29,\n                    \"prompt_tokens_details\": {\"cached_tokens\": 0, \"audio_tokens\": 0},\n                    \"completion_tokens_details\": {\n                        \"reasoning_tokens\": 0,\n                        \"audio_tokens\": 0,\n                        \"accepted_prediction_tokens\": 0,\n                        \"rejected_prediction_tokens\": 0,\n                    },\n                },\n                \"service_tier\": \"default\",\n            }\n\n            # await self.emit_status(__event_emitter__, \"info\", \"Complete\", True)\n            return result\n        except Exception as e:\n            await self.emit_status(__event_emitter__, \"error\", str(e), True)\n            formatted_response = {\n                \"id\": \"error\",\n                \"model\": \"error\",\n                \"created\": \"2023-10-01T00:00:00Z\",\n                # \"usage\": response[\"usage\"],\n                # \"object\": response[\"object\"],\n                \"choices\": [\n                    {\n                        \"index\": \"1\",\n                        # \"finish_reason\": choice[\"finish_reason\"],\n                        \"message\": {\n                            \"role\": \"assistant\",\n                            \"content\": f\"Error: {e}\",\n                        },\n                        # \"delta\": {\"role\": \"assistant\", \"content\": \"\"},\n                    }\n                ],\n            }\n            return formatted_response\n",
    "meta": {
      "description": "Talks to Language Model Gateway",
      "manifest": {
        "title": "LangChain Pipe Function",
        "author": "Colby Sawyer @ Attollo LLC (mailto:colby.sawyer@attollodefense.com)",
        "author_url": "https://github.com/ColbySawyer7",
        "version": "0.1.0"
      }
    },
    "is_active": true,
    "is_global": false,
    "updated_at": 1745989387,
    "created_at": 1745989309
  }
]
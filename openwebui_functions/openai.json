[
  {
    "id": "language_model_gateway",
    "user_id": "6c03bf27-dbbf-44c1-b980-2c6a4608f712",
    "name": "language_model_gateway",
    "type": "pipe",
    "content": "\"\"\"title: LangChain Pipe Function (Streaming Version)\nauthor: Imran Qureshi @ b.well Connected Health (mailto:imran.qureshi@bwell.com)\nauthor_url: https://github.com/imranq2\nversion: 0.2.0\nThis module defines a Pipe class that reads the oauth_id_token from the request cookies and uses it in Authorization header\nto make requests to the OpenAI API. It supports both streaming and non-streaming responses.\n\"\"\"\n\nimport asyncio\nimport json\nimport logging\nimport os\nimport time\nfrom pathlib import PurePosixPath\nfrom typing import AsyncGenerator\nfrom typing import Optional, Callable, Awaitable, Any, Dict\nfrom typing import Union, Generator, Iterator\n\nimport requests\nfrom pydantic import BaseModel\nfrom pydantic import Field\nfrom starlette.datastructures import MutableHeaders\nfrom starlette.requests import Request\nfrom urllib.parse import urlparse, urlunparse\n\nlogger = logging.getLogger(__name__)\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        emit_interval: float = Field(\n            default=2.0, description=\"Interval in seconds between status emissions\"\n        )\n        enable_status_indicator: bool = Field(\n            default=True, description=\"Enable or disable status indicator emissions\"\n        )\n        OPENAI_API_BASE_URL: str | None = Field(\n            default=None,\n            description=\"Base URL for OpenAI API, e.g., https://api.openai.com/v1\",\n        )\n\n    def __init__(self) -> None:\n        self.type: str = \"pipe\"\n        self.id: str = \"language_model_gateway\"\n        openai_api_base_url_ = self.read_base_url()\n        self.valves = self.Valves(OPENAI_API_BASE_URL=openai_api_base_url_)\n        self.name: str = \"Aiden: \"\n        self.last_emit_time: float = 0\n        self.pipelines = self.get_models()\n\n    # noinspection PyMethodMayBeStatic\n    def read_base_url(self) -> Optional[str]:\n        \"\"\"\n        Reads the OpenAI API base URL from environment variables.\n        Returns:\n            The OpenAI API base URL if set, otherwise None.\n        \"\"\"\n        return os.getenv(\"LANGUAGE_MODEL_GATEWAY_API_BASE_URL\") or os.getenv(\n            \"OPENAI_API_BASE_URL\"\n        )\n\n    # noinspection PyMethodMayBeStatic\n    async def on_startup(self) -> None:\n        # This function is called when the server is started.\n        logger.debug(f\"on_startup:{__name__}\")\n        pass\n\n    # noinspection PyMethodMayBeStatic\n    async def on_shutdown(self) -> None:\n        # This function is called when the server is stopped.\n        logger.debug(f\"on_shutdown:{__name__}\")\n        pass\n\n    async def on_valves_updated(self) -> None:\n        # This function is called when the valves are updated.\n        logger.debug(f\"on_valves_updated:{__name__}\")\n        self.pipelines = self.get_models()\n        pass\n\n    async def emit_status(\n        self,\n        __event_emitter__: Optional[Callable[[Dict[str, Any]], Awaitable[None]]],\n        level: str,\n        message: str,\n        done: bool,\n    ) -> None:\n        current_time = time.time()\n        if (\n            __event_emitter__\n            and self.valves.enable_status_indicator\n            and (\n                current_time - self.last_emit_time >= self.valves.emit_interval or done\n            )\n        ):\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": \"complete\" if done else \"in_progress\",\n                        \"level\": level,\n                        \"description\": message,\n                        \"done\": done,\n                    },\n                }\n            )\n            self.last_emit_time = current_time\n\n    async def stream_hardcoded_response(\n        self,\n        *,\n        body: Dict[str, Any],\n        __request__: Optional[Request] = None,\n        __user__: Optional[Dict[str, Any]] = None,\n        __event_emitter__: Callable[[Dict[str, Any]], Awaitable[None]] | None = None,\n        __event_call__: Callable[[Dict[str, Any]], Awaitable[Dict[str, Any]]]\n        | None = None,\n    ) -> AsyncGenerator[str, None]:\n        \"\"\"\n        Async generator to stream response chunks\n        \"\"\"\n        try:\n            await self.emit_status(\n                __event_emitter__,\n                \"info\",\n                f\"/initiating Chain: headers={__request__.headers if __request__ else None}\"\n                f\", cookies={__request__.cookies if __request__ else None}\"\n                f\" {__user__=} {body=}\",\n                False,\n            )\n\n            if __request__ is None or __user__ is None:\n                raise ValueError(\"Request and user information must be provided.\")\n\n            # Simulate streaming response\n            # Generate chunks in OpenAI streaming format\n            chunks = [\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\"role\": \"assistant\"},\n                            \"finish_reason\": None,\n                        }\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\"content\": \"Here\"},\n                            \"finish_reason\": None,\n                        }\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\"index\": 0, \"delta\": {\"content\": \" is\"}, \"finish_reason\": None}\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\"index\": 0, \"delta\": {\"content\": \" a\"}, \"finish_reason\": None}\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\"content\": \" streamed\"},\n                            \"finish_reason\": None,\n                        }\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\n                                \"content\": f\"\\nheaders=\\n{__request__.headers}\\ncookies=\\n{__request__.cookies}\\n{__user__=}\\n{body=}\",\n                            },\n                            \"finish_reason\": None,\n                        }\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\n                                \"content\": f\"\\nOAuth_id_token:\\n{__request__.cookies.get('oauth_id_token')}\\n\",\n                            },\n                            \"finish_reason\": None,\n                        }\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [{\"index\": 0, \"delta\": {}, \"finish_reason\": \"stop\"}],\n                },\n            ]\n\n            for chunk in chunks:\n                # Yield each chunk as a JSON-encoded string with a data: prefix\n                yield f\"data: {json.dumps(chunk)}\\n\\n\"\n                await self.emit_status(__event_emitter__, \"info\", \"Streaming...\", False)\n                await asyncio.sleep(0.5)  # Simulate streaming delay\n\n            await self.emit_status(__event_emitter__, \"info\", \"Stream Complete\", True)\n\n        except Exception as e:\n            error_chunk = {\n                \"id\": \"chatcmpl-error\",\n                \"object\": \"chat.completion.chunk\",\n                \"created\": int(time.time()),\n                \"model\": \"error\",\n                \"choices\": [\n                    {\n                        \"index\": 0,\n                        \"delta\": {\"content\": f\"Error: {str(e)}\"},\n                        \"finish_reason\": \"stop\",\n                    }\n                ],\n            }\n            yield f\"data: {json.dumps(error_chunk)}\\n\\n\"\n            await self.emit_status(__event_emitter__, \"error\", str(e), True)\n\n    @classmethod\n    def pathlib_url_join(cls, base_url: str, path: str) -> str:\n        \"\"\"\n        Join URLs using pathlib for path manipulation.\n\n        Args:\n            base_url: The base URL\n            path: Path to append\n\n        Returns:\n            Fully constructed URL\n        \"\"\"\n        # Parse the base URL\n        parsed_base = urlparse(base_url)\n\n        # Use PurePosixPath to handle path joining\n        full_path = str(PurePosixPath(parsed_base.path) / path.lstrip(\"/\"))\n\n        # Reconstruct the URL\n        reconstructed_url = urlunparse(\n            (\n                parsed_base.scheme,\n                parsed_base.netloc,\n                full_path,\n                parsed_base.params,\n                parsed_base.query,\n                parsed_base.fragment,\n            )\n        )\n\n        return reconstructed_url\n\n    # noinspection PyMethodMayBeStatic\n    async def pipe(\n        self,\n        body: Dict[str, Any],\n        __request__: Optional[Request] = None,\n        __user__: Optional[Dict[str, Any]] = None,\n        __event_emitter__: Optional[Callable[[Dict[str, Any]], Awaitable[None]]] = None,\n        __event_call__: Optional[\n            Callable[[Dict[str, Any]], Awaitable[Dict[str, Any]]]\n        ] = None,\n    ) -> Union[str, Generator[Any, None, None] | Iterator[Any]]:\n        \"\"\"\n        Main pipe method supporting both streaming and non-streaming responses\n        \"\"\"\n        # This is where you can add your custom pipelines like RAG.\n        logger.debug(f\"pipe:{__name__}\")\n\n        logger.debug(\"=== body ===\")\n        logger.debug(body)\n        logger.debug(\"==== End of body ===\")\n        logger.debug(f\"__request__: {__request__}\")\n        logger.debug(f\"__user__: {__user__}\")\n        logger.debug(\"==== Request Url ====\")\n        logger.debug(__request__.url if __request__ else \"No request URL provided\")\n        logger.debug(\"==== End of Request Url ====\")\n\n        assert __request__ is not None, \"Request object must be provided.\"\n\n        # logger.debug the Authorization header if available\n        auth_header = __request__.headers.get(\"Authorization\")\n        if auth_header:\n            logger.debug(f\"Authorization header: {auth_header}\")\n        else:\n            logger.debug(\"No Authorization header found.\")\n\n        auth_token: str | None = __request__.cookies.get(\"oauth_id_token\")\n        logger.debug(f\"auth_token: {auth_token}\")\n\n        open_api_base_url: str | None = self.valves.OPENAI_API_BASE_URL\n        if open_api_base_url is None:\n            logger.debug(\n                \"LanguageModelGateway:Pipes OPENAI_API_BASE_URL is not set in valves, trying environment variable.\"\n            )\n            open_api_base_url = self.read_base_url()\n            logger.debug(\n                f\"LanguageModelGateway:Pipes after trying environment variable OpenAI API_BASE_URL: {open_api_base_url}\"\n            )\n        assert open_api_base_url is not None, (\n            \"LanguageModelGateway:Pipes OpenAI_API_BASE_URL must be set as an environment variable.\"\n        )\n        assert open_api_base_url is not None, (\n            \"LanguageModelGateway:Pipe OpenAI_API_BASE_URL must be set as an environment variable.\"\n        )\n        logger.debug(f\"open_api_base_url: {open_api_base_url}\")\n\n        headers: MutableHeaders = __request__.headers.mutablecopy()\n\n        # if auth token is available in the cookies, add it to the Authorization header\n        if auth_token:\n            headers[\"Authorization\"] = f\"Bearer {auth_token}\"\n\n        # Extract model id from the model name\n        model_id = body[\"model\"][body[\"model\"].find(\".\") + 1 :]\n\n        # Update the model id in the body\n        payload = {**body, \"model\": model_id}\n\n        try:\n            # replace host with the OpenAI API base URL.  use proper urljoin to handle paths correctly\n            # include any query parameters in the URL\n            operation_path = str(__request__.url).replace(\n                \"https://open-webui.localhost/api/\", \"\"\n            )\n            logger.debug(f\"operation_path: {operation_path}\")\n            url = self.pathlib_url_join(base_url=open_api_base_url, path=operation_path)\n\n            logger.debug(f\"Calling chat completion url: {url}\")\n\n            # now run the __request__ with the OpenAI API\n            response = requests.post(\n                url=url,\n                json=payload,\n                headers=headers,\n                stream=body.get(\"stream\", False),\n                timeout=30,  # Set a timeout for the request\n            )\n\n            response.raise_for_status()\n\n            if body[\"stream\"]:\n                return response.iter_lines()\n            else:\n                return response.json()  # type: ignore[no-any-return]\n        except Exception as e:\n            # logger.error(f\"Error in pipe: {e}\")\n            # logger.debug(f\"Error details: {e.__traceback__}\")\n            return f\"Error: {e}\"\n\n    def get_models(self) -> list[dict[str, str]]:\n        open_api_base_url: str | None = self.valves.OPENAI_API_BASE_URL\n        if open_api_base_url is None:\n            logger.debug(\n                \"LanguageModelGateway:Pipes OPENAI_API_BASE_URL is not set in valves, trying environment variable.\"\n            )\n            open_api_base_url = self.read_base_url()\n            logger.debug(\n                f\"LanguageModelGateway:Pipes after trying environment variable OpenAI API_BASE_URL: {open_api_base_url}\"\n            )\n        if open_api_base_url is None:\n            return []\n        assert open_api_base_url is not None, (\n            \"LanguageModelGateway:Pipes OpenAI_API_BASE_URL must be set as an environment variable.\"\n        )\n        model_url = self.pathlib_url_join(base_url=open_api_base_url, path=\"models\")\n        # call the models endpoint to get the list of available models\n        logger.debug(f\"Calling models endpoint: {model_url}\")\n        response = requests.get(model_url, timeout=30)  # Set a timeout for the request\n        response.raise_for_status()\n        models = response.json().get(\"data\", [])\n        logger.debug(f\"Received models from {model_url}: {models}\")\n        return [\n            {\n                \"id\": model[\"id\"],\n                \"name\": model[\"id\"],\n            }\n            for model in models\n        ]\n\n    def pipes(self) -> list[dict[str, str]]:\n        return self.pipelines\n",
    "meta": {
      "description": "Talks to Language Model Gateway and passes the OAuth ID token in the request cookies as Bearer Authorization header.",
      "manifest": {
        "title": "Language Model Gateway Pipe",
        "author": "Imran Qureshi @ b.well Connected Health (mailto:imran.qureshi@bwell.com)",
        "author_url": "https://github.com/imranq2",
        "version": "0.1.0"
      }
    },
    "is_active": true,
    "is_global": true,
    "updated_at": 1745989387,
    "created_at": 1745989309
  }
]
[
  {
    "id": "language_model_gateway",
    "user_id": "6c03bf27-dbbf-44c1-b980-2c6a4608f712",
    "name": "language_model_gateway",
    "type": "pipe",
    "content": "\"\"\"title: LangChain Pipe Function (Streaming Version)\nauthor: Imran Qureshi @ b.well Connected Health (mailto:imran.qureshi@bwell.com)\nauthor_url: https://github.com/imranq2\nversion: 0.2.0\nThis module defines a Pipe class that reads the oauth_id_token from the request cookies and uses it in Authorization header\nto make requests to the OpenAI API. It supports both streaming and non-streaming responses.\n\"\"\"\n\nimport asyncio\nimport json\nimport logging\nimport os\nimport time\nfrom pathlib import PurePosixPath\nfrom typing import AsyncGenerator, List\nfrom typing import Optional, Callable, Awaitable, Any, Dict\nimport httpx\n\nfrom pydantic import BaseModel\nfrom pydantic import Field\nfrom starlette.datastructures import MutableHeaders\nfrom starlette.requests import Request\nfrom urllib.parse import urlparse, urlunparse\n\nlogger = logging.getLogger(__name__)\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        emit_interval: float = Field(\n            default=2.0, description=\"Interval in seconds between status emissions\"\n        )\n        enable_status_indicator: bool = Field(\n            default=True, description=\"Enable or disable status indicator emissions\"\n        )\n        OPENAI_API_BASE_URL: str | None = Field(\n            default=None,\n            description=\"Base URL for OpenAI API, e.g., https://api.openai.com/v1\",\n        )\n        model_name_prefix: str = Field(\n            default=\"MCP: \",\n            description=\"Prefix for model names in the dropdown\",\n        )\n        restrict_to_admins: bool = Field(\n            default=False,\n            description=\"Restrict access to this pipe to admin users only\",\n        )\n        restrict_to_model_ids: list[str] = Field(\n            default_factory=list,\n            description=\"List of model IDs to restrict access to. If empty, no restriction is applied.\",\n        )\n\n    def __init__(self) -> None:\n        self.type: str = \"pipe\"\n        self.id: str = \"language_model_gateway\"\n        openai_api_base_url_ = self.read_base_url()\n        self.valves = self.Valves(OPENAI_API_BASE_URL=openai_api_base_url_)\n        self.name: str = self.valves.model_name_prefix\n        self.last_emit_time: float = 0\n        self.pipelines: List[Dict[str, Any]] | None = None\n\n    # noinspection PyMethodMayBeStatic\n    def read_base_url(self) -> Optional[str]:\n        \"\"\"\n        Reads the OpenAI API base URL from environment variables.\n        Returns:\n            The OpenAI API base URL if set, otherwise None.\n        \"\"\"\n        return os.getenv(\"LANGUAGE_MODEL_GATEWAY_API_BASE_URL\") or os.getenv(\n            \"OPENAI_API_BASE_URL\"\n        )\n\n    # noinspection PyMethodMayBeStatic\n    async def on_startup(self) -> None:\n        # This function is called when the server is started.\n        logger.debug(f\"on_startup:{__name__}\")\n        self.pipelines = await self.get_models()\n        pass\n\n    # noinspection PyMethodMayBeStatic\n    async def on_shutdown(self) -> None:\n        # This function is called when the server is stopped.\n        logger.debug(f\"on_shutdown:{__name__}\")\n        pass\n\n    async def on_valves_updated(self) -> None:\n        # This function is called when the valves are updated.\n        logger.debug(f\"on_valves_updated:{__name__}\")\n        self.pipelines = await self.get_models()\n        pass\n\n    async def emit_status(\n        self,\n        __event_emitter__: Optional[Callable[[Dict[str, Any]], Awaitable[None]]],\n        level: str,\n        message: str,\n        done: bool,\n    ) -> None:\n        current_time = time.time()\n        if (\n            __event_emitter__\n            and self.valves.enable_status_indicator\n            and (\n                current_time - self.last_emit_time >= self.valves.emit_interval or done\n            )\n        ):\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": \"complete\" if done else \"in_progress\",\n                        \"level\": level,\n                        \"description\": message,\n                        \"done\": done,\n                    },\n                }\n            )\n            self.last_emit_time = current_time\n\n    async def stream_hardcoded_response(\n        self,\n        *,\n        body: Dict[str, Any],\n        __request__: Optional[Request] = None,\n        __user__: Optional[Dict[str, Any]] = None,\n        __event_emitter__: Callable[[Dict[str, Any]], Awaitable[None]] | None = None,\n        __event_call__: Callable[[Dict[str, Any]], Awaitable[Dict[str, Any]]]\n        | None = None,\n    ) -> AsyncGenerator[str, None]:\n        \"\"\"\n        Async generator to stream response chunks\n        \"\"\"\n        try:\n            await self.emit_status(\n                __event_emitter__,\n                \"info\",\n                f\"/initiating Chain: headers={__request__.headers if __request__ else None}\"\n                f\", cookies={__request__.cookies if __request__ else None}\"\n                f\" {__user__=} {body=}\",\n                False,\n            )\n\n            if __request__ is None or __user__ is None:\n                raise ValueError(\"Request and user information must be provided.\")\n\n            # Simulate streaming response\n            # Generate chunks in OpenAI streaming format\n            chunks = [\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\"role\": \"assistant\"},\n                            \"finish_reason\": None,\n                        }\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\"content\": \"Here\"},\n                            \"finish_reason\": None,\n                        }\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\"index\": 0, \"delta\": {\"content\": \" is\"}, \"finish_reason\": None}\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\"index\": 0, \"delta\": {\"content\": \" a\"}, \"finish_reason\": None}\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\"content\": \" streamed\"},\n                            \"finish_reason\": None,\n                        }\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\n                                \"content\": f\"\\nheaders=\\n{__request__.headers}\\ncookies=\\n{__request__.cookies}\\n{__user__=}\\n{body=}\",\n                            },\n                            \"finish_reason\": None,\n                        }\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\n                                \"content\": f\"\\nOAuth_id_token:\\n{__request__.cookies.get('oauth_id_token')}\\n\",\n                            },\n                            \"finish_reason\": None,\n                        }\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [{\"index\": 0, \"delta\": {}, \"finish_reason\": \"stop\"}],\n                },\n            ]\n\n            for chunk in chunks:\n                # Yield each chunk as a JSON-encoded string with a data: prefix\n                yield f\"data: {json.dumps(chunk)}\\n\\n\"\n                await self.emit_status(__event_emitter__, \"info\", \"Streaming...\", False)\n                await asyncio.sleep(0.5)  # Simulate streaming delay\n\n            await self.emit_status(__event_emitter__, \"info\", \"Stream Complete\", True)\n\n        except Exception as e:\n            error_chunk = {\n                \"id\": \"chatcmpl-error\",\n                \"object\": \"chat.completion.chunk\",\n                \"created\": int(time.time()),\n                \"model\": \"error\",\n                \"choices\": [\n                    {\n                        \"index\": 0,\n                        \"delta\": {\"content\": f\"Error: {str(e)}\"},\n                        \"finish_reason\": \"stop\",\n                    }\n                ],\n            }\n            yield f\"data: {json.dumps(error_chunk)}\\n\\n\"\n            await self.emit_status(__event_emitter__, \"error\", str(e), True)\n\n    @classmethod\n    def pathlib_url_join(cls, base_url: str, path: str) -> str:\n        \"\"\"\n        Join URLs using pathlib for path manipulation.\n\n        Args:\n            base_url: The base URL\n            path: Path to append\n\n        Returns:\n            Fully constructed URL\n        \"\"\"\n        # Parse the base URL\n        parsed_base = urlparse(base_url)\n\n        # Use PurePosixPath to handle path joining\n        full_path = str(PurePosixPath(parsed_base.path) / path.lstrip(\"/\"))\n\n        # Reconstruct the URL\n        reconstructed_url = urlunparse(\n            (\n                parsed_base.scheme,\n                parsed_base.netloc,\n                full_path,\n                parsed_base.params,\n                parsed_base.query,\n                parsed_base.fragment,\n            )\n        )\n\n        return reconstructed_url\n\n    # noinspection PyMethodMayBeStatic\n    async def pipe(\n        self,\n        body: Dict[str, Any],\n        __request__: Optional[Request] = None,\n        __user__: Optional[Dict[str, Any]] = None,\n        __event_emitter__: Optional[Callable[[Dict[str, Any]], Awaitable[None]]] = None,\n        __event_call__: Optional[\n            Callable[[Dict[str, Any]], Awaitable[Dict[str, Any]]]\n        ] = None,\n    ) -> AsyncGenerator[str, None]:\n        \"\"\"\n        Main pipe method supporting both streaming and non-streaming responses\n        \"\"\"\n        # This is where you can add your custom pipelines like RAG.\n        logger.debug(f\"pipe:{__name__}\")\n\n        logger.debug(\"=== body ===\")\n        logger.debug(body)\n        logger.debug(\"==== End of body ===\")\n        logger.debug(f\"__request__: {__request__}\")\n        logger.debug(f\"__user__: {__user__}\")\n        logger.debug(\"==== Request Url ====\")\n        logger.debug(__request__.url if __request__ else \"No request URL provided\")\n        logger.debug(\"==== End of Request Url ====\")\n\n        assert __request__ is not None, \"Request object must be provided.\"\n\n        # logger.debug the Authorization header if available\n        auth_header = __request__.headers.get(\"Authorization\")\n        if auth_header:\n            logger.debug(f\"Authorization header: {auth_header}\")\n        else:\n            logger.debug(\"No Authorization header found.\")\n\n        auth_token: str | None = __request__.cookies.get(\"oauth_id_token\")\n        logger.debug(f\"auth_token: {auth_token}\")\n\n        open_api_base_url: str | None = self.valves.OPENAI_API_BASE_URL\n        if open_api_base_url is None:\n            logger.debug(\n                \"LanguageModelGateway::pipe OPENAI_API_BASE_URL is not set in valves, trying environment variable.\"\n            )\n            open_api_base_url = self.read_base_url()\n            logger.debug(\n                f\"LanguageModelGateway::pipe after trying environment variable OpenAI API_BASE_URL: {open_api_base_url}\"\n            )\n        assert open_api_base_url is not None, (\n            \"LanguageModelGateway::pipe OpenAI_API_BASE_URL must be set as an environment variable.\"\n        )\n        assert open_api_base_url is not None, (\n            \"LanguageModelGateway::pipe OpenAI_API_BASE_URL must be set as an environment variable.\"\n        )\n        logger.debug(f\"open_api_base_url: {open_api_base_url}\")\n\n        headers: MutableHeaders = __request__.headers.mutablecopy()\n        # remove Content-Length header if it exists so we calculate it correctly\n        del headers[\"Content-Length\"]\n\n        # if auth token is available in the cookies, add it to the Authorization header\n        if auth_token:\n            headers[\"Authorization\"] = f\"Bearer {auth_token}\"\n\n        # Extract model id from the model name\n        model_id = body[\"model\"][body[\"model\"].find(\".\") + 1 :]\n\n        # Update the model id in the body\n        payload = {**body, \"model\": model_id}\n\n        try:\n            # replace host with the OpenAI API base URL.  use proper urljoin to handle paths correctly\n            # include any query parameters in the URL\n            operation_path = str(__request__.url).replace(\n                \"https://open-webui.localhost/api/\", \"\"\n            )\n            logger.debug(f\"operation_path: {operation_path}\")\n            url = self.pathlib_url_join(base_url=open_api_base_url, path=operation_path)\n\n            logger.debug(\n                f\"LanguageModelGateway::pipe Calling chat completion url: {url} with payload: {payload} and headers: {headers}\"\n            )\n\n            # now run the __request__ with the OpenAI API\n            # Use httpx.stream for more efficient streaming\n            async with httpx.AsyncClient() as client:\n                async with client.stream(\n                    method=\"POST\", url=url, json=payload, headers=headers, timeout=30.0\n                ) as response:\n                    # Raise an exception for HTTP errors\n                    response.raise_for_status()\n\n                    # Handle streaming or regular response\n                    if body.get(\"stream\", False):\n                        # Stream mode: yield lines as they arrive\n                        async for line in response.aiter_lines():\n                            yield line\n                    else:\n                        # Non-streaming mode: collect and return full JSON response\n                        yield await response.json()\n        except Exception as e:\n            # logger.error(f\"Error in pipe: {e}\")\n            # logger.debug(f\"Error details: {e.__traceback__}\")\n            yield f\"LanguageModelGateway::pipe Error: {e}\"\n\n    async def get_models(self) -> list[dict[str, str]]:\n        \"\"\"\n        Fetches the list of available models from the OpenAI API.\n        Returns:\n            A list of dictionaries containing model IDs and names.\n\n        \"\"\"\n        open_api_base_url: str | None = self.valves.OPENAI_API_BASE_URL\n        if open_api_base_url is None:\n            logger.debug(\n                \"LanguageModelGateway:Pipes OPENAI_API_BASE_URL is not set in valves, trying environment variable.\"\n            )\n            open_api_base_url = self.read_base_url()\n            logger.debug(\n                f\"LanguageModelGateway:Pipes after trying environment variable OpenAI API_BASE_URL: {open_api_base_url}\"\n            )\n        if open_api_base_url is None:\n            return []\n        assert open_api_base_url is not None, (\n            \"LanguageModelGateway:Pipes OpenAI_API_BASE_URL must be set as an environment variable.\"\n        )\n        model_url = self.pathlib_url_join(base_url=open_api_base_url, path=\"models\")\n        # call the models endpoint to get the list of available models\n        logger.debug(f\"Calling models endpoint: {model_url}\")\n        models: list[dict[str, str]] = []\n        async with httpx.AsyncClient() as client:\n            # Perform the GET request with a timeout\n            response = await client.get(\n                url=model_url,\n                timeout=30.0,  # 30 seconds timeout\n            )\n\n            # Raise an exception for HTTP errors\n            response.raise_for_status()\n\n            # Parse JSON and extract 'data' key, defaulting to empty list\n            models = response.json().get(\"data\", [])\n        logger.debug(f\"Received models from {model_url}: {models}\")\n        if self.valves.restrict_to_model_ids:\n            # Filter models based on the restricted model IDs\n            models = [\n                model\n                for model in models\n                if model[\"id\"] in self.valves.restrict_to_model_ids\n            ]\n            logger.debug(f\"Filtered models: {models}\")\n        return [\n            {\n                \"id\": model[\"id\"],\n                \"name\": model[\"id\"],\n            }\n            for model in models\n        ]\n\n    async def pipes(self) -> list[dict[str, str]]:\n        if self.pipelines is None:\n            logger.debug(\"Fetching models for the first time.\")\n            self.pipelines = await self.get_models()\n        return self.pipelines or []\n",
    "meta": {
      "description": "Talks to Language Model Gateway and passes the OAuth ID token in the request cookies as Bearer Authorization header.",
      "manifest": {
        "title": "Language Model Gateway Pipe",
        "author": "Imran Qureshi @ b.well Connected Health (mailto:imran.qureshi@bwell.com)",
        "author_url": "https://github.com/imranq2",
        "version": "0.1.0"
      }
    },
    "is_active": true,
    "is_global": true,
    "updated_at": 1745989387,
    "created_at": 1745989309
  }
]
[
  {
    "id": "language_model_gateway",
    "user_id": "6c03bf27-dbbf-44c1-b980-2c6a4608f712",
    "name": "language_model_gateway",
    "type": "pipe",
    "content": "\"\"\"title: LangChain Pipe Function (Streaming Version)\nauthor: Imran Qureshi @ b.well Connected Health (mailto:imran.qureshi@bwell.com)\nauthor_url: https://github.com/imranq2\nversion: 0.2.0\nThis module defines a Pipe class that reads the oauth_id_token from the request cookies and uses it in Authorization header\nto make requests to the OpenAI API. It supports both streaming and non-streaming responses.\n\"\"\"\n\nimport asyncio\nimport datetime\nimport json\nimport logging\nimport os\nimport time\nfrom pathlib import PurePosixPath\nfrom typing import AsyncGenerator, List\nfrom typing import Optional, Callable, Awaitable, Any, Dict\nfrom urllib.parse import urlparse, urlunparse\n\nimport httpx\nfrom pydantic import BaseModel\nfrom pydantic import Field\nfrom starlette.requests import Request\n\nlogger = logging.getLogger(__name__)\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        emit_interval: float = Field(\n            default=2.0, description=\"Interval in seconds between status emissions\"\n        )\n        enable_status_indicator: bool = Field(\n            default=True, description=\"Enable or disable status indicator emissions\"\n        )\n        OPENAI_API_BASE_URL: str | None = Field(\n            default=None,\n            description=\"Base URL for OpenAI API, e.g., https://api.openai.com/v1\",\n        )\n        model_name_prefix: str = Field(\n            default=\"MCP: \",\n            description=\"Prefix for model names in the dropdown\",\n        )\n        restrict_to_admins: bool = Field(\n            default=False,\n            description=\"Restrict access to this pipe to admin users only\",\n        )\n        restrict_to_model_ids: list[str] = Field(\n            default_factory=list,\n            description=\"List of model IDs to restrict access to. If empty, no restriction is applied.\",\n        )\n        debug_mode: bool = Field(\n            default=False,\n            description=\"Enable debug mode for additional logging and debugging information\",\n        )\n\n    def __init__(self) -> None:\n        self.type: str = \"pipe\"\n        self.id: str = \"language_model_gateway\"\n        openai_api_base_url_ = self.read_base_url()\n        self.valves = self.Valves(OPENAI_API_BASE_URL=openai_api_base_url_)\n        self.name: str = self.valves.model_name_prefix\n        self.last_emit_time: float = 0\n        self.pipelines: List[Dict[str, Any]] | None = None\n\n    # noinspection PyMethodMayBeStatic\n    def read_base_url(self) -> Optional[str]:\n        \"\"\"\n        Reads the OpenAI API base URL from environment variables.\n        Returns:\n            The OpenAI API base URL if set, otherwise None.\n        \"\"\"\n        return os.getenv(\"LANGUAGE_MODEL_GATEWAY_API_BASE_URL\") or os.getenv(\n            \"OPENAI_API_BASE_URL\"\n        )\n\n    # noinspection PyMethodMayBeStatic\n    async def on_startup(self) -> None:\n        # This function is called when the server is started.\n        logger.debug(f\"on_startup:{__name__}\")\n        self.pipelines = await self.get_models()\n        pass\n\n    # noinspection PyMethodMayBeStatic\n    async def on_shutdown(self) -> None:\n        # This function is called when the server is stopped.\n        logger.debug(f\"on_shutdown:{__name__}\")\n        pass\n\n    async def on_valves_updated(self) -> None:\n        # This function is called when the valves are updated.\n        logger.debug(f\"on_valves_updated:{__name__}\")\n        self.pipelines = await self.get_models()\n        pass\n\n    async def emit_status(\n        self,\n        __event_emitter__: Optional[Callable[[Dict[str, Any]], Awaitable[None]]],\n        level: str,\n        message: str,\n        done: bool,\n        message_type: str = \"status\",\n    ) -> None:\n        \"\"\"\n        Emit status updates at controlled intervals\n        Args:\n            __event_emitter__: Callable to emit events\n            level: Status level (e.g., \"info\", \"error\")\n            message: Status message\n            done: Whether the operation is complete\n            message_type: Type of message, default is \"status\".  https://docs.openwebui.com/features/plugin/tools/development#status\n        Returns:\n            None\n        \"\"\"\n        current_time = time.time()\n        if (\n            __event_emitter__\n            and self.valves.enable_status_indicator\n            and (\n                current_time - self.last_emit_time >= self.valves.emit_interval or done\n            )\n        ):\n            # https://docs.openwebui.com/features/plugin/tools/development#event-emitters\n            # Interactive events: https://docs.openwebui.com/features/plugin/events/#interactive-events\n            await __event_emitter__(\n                {\n                    \"type\": message_type,\n                    \"data\": {\n                        \"status\": \"complete\" if done else \"in_progress\",\n                        \"level\": level,\n                        \"description\": message,\n                        \"done\": done,\n                    },\n                }\n            )\n            self.last_emit_time = current_time\n\n    async def stream_hardcoded_response(\n        self,\n        *,\n        body: Dict[str, Any],\n        __request__: Optional[Request] = None,\n        __user__: Optional[Dict[str, Any]] = None,\n        __event_emitter__: Callable[[Dict[str, Any]], Awaitable[None]] | None = None,\n        __event_call__: Callable[[Dict[str, Any]], Awaitable[Dict[str, Any]]]\n        | None = None,\n    ) -> AsyncGenerator[str, None]:\n        \"\"\"\n        Async generator to stream response chunks\n        \"\"\"\n        try:\n            await self.emit_status(\n                __event_emitter__,\n                \"info\",\n                f\"/initiating Chain: headers={__request__.headers if __request__ else None}\"\n                f\", cookies={__request__.cookies if __request__ else None}\"\n                f\" {__user__=} {body=}\",\n                False,\n            )\n\n            if __request__ is None or __user__ is None:\n                raise ValueError(\"Request and user information must be provided.\")\n\n            # Simulate streaming response\n            # Generate chunks in OpenAI streaming format\n            chunks = [\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\"role\": \"assistant\"},\n                            \"finish_reason\": None,\n                        }\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\"content\": \"Here\"},\n                            \"finish_reason\": None,\n                        }\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\"index\": 0, \"delta\": {\"content\": \" is\"}, \"finish_reason\": None}\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\"index\": 0, \"delta\": {\"content\": \" a\"}, \"finish_reason\": None}\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\"content\": \" streamed\"},\n                            \"finish_reason\": None,\n                        }\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\n                                \"content\": f\"\\nheaders=\\n{__request__.headers}\\ncookies=\\n{__request__.cookies}\\n{__user__=}\\n{body=}\",\n                            },\n                            \"finish_reason\": None,\n                        }\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\n                                \"content\": f\"\\nOAuth_id_token:\\n{__request__.cookies.get('oauth_id_token')}\\n\",\n                            },\n                            \"finish_reason\": None,\n                        }\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [{\"index\": 0, \"delta\": {}, \"finish_reason\": \"stop\"}],\n                },\n            ]\n\n            for chunk in chunks:\n                # Yield each chunk as a JSON-encoded string with a data: prefix\n                yield f\"data: {json.dumps(chunk)}\\n\\n\"\n                await self.emit_status(__event_emitter__, \"info\", \"Streaming...\", False)\n                await asyncio.sleep(0.5)  # Simulate streaming delay\n\n            await self.emit_status(__event_emitter__, \"info\", \"Stream Complete\", True)\n\n        except Exception as e:\n            error_chunk = {\n                \"id\": \"chatcmpl-error\",\n                \"object\": \"chat.completion.chunk\",\n                \"created\": int(time.time()),\n                \"model\": \"error\",\n                \"choices\": [\n                    {\n                        \"index\": 0,\n                        \"delta\": {\"content\": f\"Error: {str(e)}\"},\n                        \"finish_reason\": \"stop\",\n                    }\n                ],\n            }\n            yield f\"data: {json.dumps(error_chunk)}\\n\\n\"\n            await self.emit_status(__event_emitter__, \"error\", str(e), True)\n\n    @classmethod\n    def pathlib_url_join(cls, base_url: str, path: str) -> str:\n        \"\"\"\n        Join URLs using pathlib for path manipulation.\n\n        Args:\n            base_url: The base URL\n            path: Path to append\n\n        Returns:\n            Fully constructed URL\n        \"\"\"\n        # Parse the base URL\n        parsed_base = urlparse(base_url)\n\n        # Use PurePosixPath to handle path joining\n        full_path = str(PurePosixPath(parsed_base.path) / path.lstrip(\"/\"))\n\n        # Reconstruct the URL\n        reconstructed_url = urlunparse(\n            (\n                parsed_base.scheme,\n                parsed_base.netloc,\n                full_path,\n                parsed_base.params,\n                parsed_base.query,\n                parsed_base.fragment,\n            )\n        )\n\n        return reconstructed_url\n\n    @staticmethod\n    def log_httpx_request(request: httpx.Request) -> str:\n        \"\"\"\n        Convert an HTTPX request to a detailed string representation.\n\n        Args:\n            request (httpx.Request): The HTTPX request to log\n\n        Returns:\n            str: Formatted string representation of the request\n        \"\"\"\n        # Construct request details\n        request_log = f\"\"\"\n    HTTPX Request:\n    - Method: {request.method}\n    - URL: {request.url}\n    - Headers: {dict(request.headers)}\n    - Body: {request.content.decode(\"utf-8\", errors=\"replace\") if request.content else \"No body\"}\n    \"\"\".strip()\n\n        return request_log\n\n    @staticmethod\n    def log_response_as_string(response1: httpx.Response) -> str:\n        \"\"\"\n        Convert an HTTPX response to a detailed, formatted string.\n\n        Args:\n            response1 (httpx.Response): The HTTP response to log\n\n        Returns:\n            str: Comprehensive response log string\n        \"\"\"\n        try:\n            # Attempt to parse JSON response\n            try:\n                response_body = json.dumps(response1.json(), indent=2)\n            except (ValueError, json.JSONDecodeError):\n                # Fallback to text if not JSON\n                response_body = response1.text[:1000]  # Limit body size\n        except Exception:\n            response_body = \"(Unable to decode response body)\"\n\n        response_log = f\"\"\"\n    HTTPX Response Log:\n    - Timestamp: {datetime.datetime.now().isoformat()}\n    - Status Code: {response1.status_code}\n    - URL: {response1.request.url}\n    - Method: {response1.request.method}\n    - Response Headers:\n    {json.dumps(dict(response1.headers), indent=2)}\n    - Response Body:\n    {response_body}\n    - Response Encoding: {response1.encoding}\n    - Response Elapsed Time: {response1.elapsed}\n    \"\"\".strip()\n\n        return response_log\n\n    # noinspection PyMethodMayBeStatic\n    async def pipe(\n        self,\n        body: Dict[str, Any],\n        __request__: Optional[Request] = None,\n        __user__: Optional[Dict[str, Any]] = None,\n        __event_emitter__: Optional[Callable[[Dict[str, Any]], Awaitable[None]]] = None,\n        __event_call__: Optional[\n            Callable[[Dict[str, Any]], Awaitable[Dict[str, Any]]]\n        ] = None,\n        __oauth_token__: Optional[Dict[str, Any]] = None,\n        __chat_id__: Optional[str] = None,\n        __session_id__: Optional[str] = None,\n        __message_id__: Optional[str] = None,\n        __metadata__: Optional[Dict[str, Any]] = None,\n        __files__: Optional[List[str]] = None,\n    ) -> AsyncGenerator[str, None]:\n        \"\"\"\n        Main pipe method supporting both streaming and non-streaming responses\n        OpenWebUI Documentation: https://docs.openwebui.com/features/plugin/functions/pipe#using-internal-open-webui-functions\n        Parameters: https://docs.openwebui.com/features/plugin/tools/development#optional-arguments\n        \"\"\"\n\n        if not __oauth_token__ or \"access_token\" not in __oauth_token__:\n            yield \"Error: User is not authenticated via OAuth or token is unavailable.\"\n            return\n\n        access_token: str | None = __oauth_token__.get(\"access_token\")\n\n        id_token: str | None = __oauth_token__.get(\"id_token\")\n\n        await self.emit_status(\n            __event_emitter__,\n            \"info\",\n            \"Working...\",\n            False,\n        )\n        logger.debug(f\"pipe:{__name__}\")\n\n        logger.debug(\"=== body ===\")\n        logger.debug(body)\n        logger.debug(\"==== End of body ===\")\n        logger.debug(f\"__request__: {__request__}\")\n        logger.debug(f\"__user__: {__user__}\")\n        logger.debug(\"==== Request Url ====\")\n        logger.debug(__request__.url if __request__ else \"No request URL provided\")\n        logger.debug(\"==== End of Request Url ====\")\n\n        assert __request__ is not None, \"Request object must be provided.\"\n\n        # logger.debug the Authorization header if available\n        auth_header = __request__.headers.get(\"Authorization\")\n        if auth_header:\n            logger.debug(f\"Authorization header: {auth_header}\")\n        else:\n            logger.debug(\"No Authorization header found.\")\n\n        if self.valves.debug_mode:\n            yield f\"User:\\n{__user__}\" + \"\\n\"\n            yield f\"Original Headers:\\n{dict(__request__.headers)}\" + \"\\n\"\n\n        open_api_base_url: str | None = self.valves.OPENAI_API_BASE_URL\n        if open_api_base_url is None:\n            logger.debug(\n                \"LanguageModelGateway::pipe OPENAI_API_BASE_URL is not set in valves, trying environment variable.\"\n            )\n            open_api_base_url = self.read_base_url()\n            logger.debug(\n                f\"LanguageModelGateway::pipe after trying environment variable OpenAI API_BASE_URL: {open_api_base_url}\"\n            )\n        assert open_api_base_url is not None, (\n            \"LanguageModelGateway::pipe OpenAI_API_BASE_URL must be set as an environment variable.\"\n        )\n        assert open_api_base_url is not None, (\n            \"LanguageModelGateway::pipe OpenAI_API_BASE_URL must be set as an environment variable.\"\n        )\n        logger.debug(f\"open_api_base_url: {open_api_base_url}\")\n\n        # Extract model id from the model name\n        model_id = body[\"model\"][body[\"model\"].find(\".\") + 1 :]\n\n        # Update the model id in the body\n        payload = {**body, \"model\": model_id}\n        if self.valves.debug_mode:\n            yield json.dumps(payload) + \"\\n\"\n\n        url = self.pathlib_url_join(base_url=open_api_base_url, path=\"chat/completions\")\n        response_text: str = \"\"\n\n        v = 11\n\n        is_streaming: bool = body.get(\"stream\", False)\n\n        try:\n            logger.debug(\n                f\"LanguageModelGateway::pipe Calling chat completion url: {url} with payload: {payload} and headers: {__request__.headers}\"\n            )\n\n            # now run the __request__ with the OpenAI API\n            # Headers\n            headers = {\n                \"Content-Type\": \"application/json\",\n                \"Authorization\": f\"Bearer {access_token}\",\n                \"X-ID-Token\": id_token if id_token else \"\",\n                \"X-Session-Id\": __session_id__ if __session_id__ else \"\",\n                \"X-Chat-Id\": __chat_id__ if __chat_id__ else \"\",\n                \"X-Message-Id\": __message_id__ if __message_id__ else \"\",\n            }\n            # set User-Agent to the one from the request, if available\n            if \"User-Agent\" in __request__.headers:\n                headers[\"User-Agent\"] = __request__.headers[\"User-Agent\"]\n            # set Referrer to the one from the request, if available\n            if \"Referrer\" in __request__.headers:\n                headers[\"Referrer\"] = __request__.headers[\"Referrer\"]\n            # set Cookie to the one from the request, if available\n            if \"Cookie\" in __request__.headers:\n                headers[\"Cookie\"] = __request__.headers[\"Cookie\"]\n            # set traceparent to the one from the request, if available\n            if \"traceparent\" in __request__.headers:\n                headers[\"traceparent\"] = __request__.headers[\"traceparent\"]\n            if \"origin\" in __request__.headers:\n                headers[\"Origin\"] = __request__.headers[\"origin\"]\n            if \"Accept-Encoding\" in __request__.headers:\n                headers[\"Accept-Encoding\"] = __request__.headers[\"Accept-Encoding\"]\n\n            # Add custom headers for OpenWebUI user information\n            if __user__ is not None:\n                user = __user__\n                # check that each value is not None before adding to headers\n                if user.get(\"name\") is not None:\n                    headers[\"X-OpenWebUI-User-Name\"] = user[\"name\"]\n                if user.get(\"id\") is not None:\n                    headers[\"X-OpenWebUI-User-Id\"] = user[\"id\"]\n                if user.get(\"email\") is not None:\n                    headers[\"X-OpenWebUI-User-Email\"] = user[\"email\"]\n                if user.get(\"role\") is not None:\n                    headers[\"X-OpenWebUI-User-Role\"] = user[\"role\"]\n                if (\n                    user.get(\"info\") is not None\n                    and isinstance(user[\"info\"], dict)\n                    and user[\"info\"].get(\"location\") is not None\n                    and isinstance(user[\"info\"][\"location\"], str)\n                ):\n                    location = user[\"info\"][\"location\"]\n                    if self.valves.debug_mode:\n                        yield \"Location: \" + type(location).__name__ + f\"{location}\\n\"\n                    headers[\"X-OpenWebUI-User-Location\"] = user[\"info\"][\"location\"]\n\n            # copy any headers that start with \"x-\"\n            for key, value in __request__.headers.items():\n                if key.lower().startswith(\"x-\"):\n                    headers[key] = value\n\n            if self.valves.debug_mode:\n                yield url + \"\\n\"\n                yield f\"New Headers: {dict(headers)}\" + \"\\n\"\n                yield json.dumps(payload) + \"\\n\"\n\n            # Use httpx.post for a plain POST request\n            async with httpx.AsyncClient() as client:\n                response = await client.post(\n                    url=url,\n                    json=payload,\n                    headers=headers,\n                    timeout=30.0,\n                    follow_redirects=True,\n                )\n                # Raise an exception for HTTP errors\n                response.raise_for_status()\n\n                # # Handle streaming or regular response\n                content_type = response.headers.get(\"content-type\", \"\")\n                if content_type.startswith(\"text/event-stream\"):\n                    # Stream mode: yield lines as they arrive\n                    async for line in response.aiter_lines():\n                        if line:\n                            yield line + \"\\n\"\n                else:\n                    # Non-streaming mode: collect and return full JSON response\n                    yield response.json()\n\n            await self.emit_status(__event_emitter__, \"info\", \"Done\", True)\n        except httpx.HTTPStatusError as e:\n            yield (\n                f\"LanguageModelGateway::pipe HTTP Status Error [{v}]:\"\n                + f\" {type(e)} {e}\\n\"\n                + f\"{self.log_httpx_request(e.request)}\\n\"\n                + f\"{self.log_response_as_string(e.response)}\"\n            )\n        except Exception as e:\n            # logger.error(f\"Error in pipe: {e}\")\n            # logger.debug(f\"Error details: {e.__traceback__}\")\n            httpx_version = httpx.__version__\n            if is_streaming:\n                error_chunk = {\n                    \"id\": \"chatcmpl-error\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"error\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\"content\": f\"Error: {str(e)}\"},\n                            \"finish_reason\": \"stop\",\n                        }\n                    ],\n                }\n                yield f\"data: {json.dumps(error_chunk)}\\n\\n\"\n            else:\n                yield f\"LanguageModelGateway::pipe Error [{v}]: {type(e)} {e} {httpx_version=} [{url=}] original=[{__request__.url}] {response_text=} {payload=}\\n\"\n\n            await self.emit_status(__event_emitter__, \"error\", str(e), True)\n\n    async def get_models(self) -> list[dict[str, str]]:\n        \"\"\"\n        Fetches the list of available models from the OpenAI API.\n        Returns:\n            A list of dictionaries containing model IDs and names.\n\n        \"\"\"\n        open_api_base_url: str | None = self.valves.OPENAI_API_BASE_URL\n        if open_api_base_url is None:\n            logger.debug(\n                \"LanguageModelGateway:Pipes OPENAI_API_BASE_URL is not set in valves, trying environment variable.\"\n            )\n            open_api_base_url = self.read_base_url()\n            logger.debug(\n                f\"LanguageModelGateway:Pipes after trying environment variable OpenAI API_BASE_URL: {open_api_base_url}\"\n            )\n        if open_api_base_url is None:\n            return []\n        assert open_api_base_url is not None, (\n            \"LanguageModelGateway:Pipes OpenAI_API_BASE_URL must be set as an environment variable.\"\n        )\n        model_url = self.pathlib_url_join(base_url=open_api_base_url, path=\"models\")\n        # call the models endpoint to get the list of available models\n        logger.debug(f\"Calling models endpoint: {model_url}\")\n        models: list[dict[str, str]] = []\n        async with httpx.AsyncClient() as client:\n            # Perform the GET request with a timeout\n            response = await client.get(\n                url=model_url,\n                timeout=30.0,  # 30 seconds timeout\n            )\n\n            # Raise an exception for HTTP errors\n            response.raise_for_status()\n\n            # Parse JSON and extract 'data' key, defaulting to empty list\n            models = response.json().get(\"data\", [])\n        logger.debug(f\"Received models from {model_url}: {models}\")\n        if self.valves.restrict_to_model_ids:\n            # Filter models based on the restricted model IDs\n            models = [\n                model\n                for model in models\n                if model[\"id\"] in self.valves.restrict_to_model_ids\n            ]\n            logger.debug(f\"Filtered models: {models}\")\n        return [\n            {\n                \"id\": model[\"id\"],\n                \"name\": model[\"id\"],\n            }\n            for model in models\n        ]\n\n    async def pipes(self) -> list[dict[str, str]]:\n        if self.pipelines is None:\n            logger.debug(\"Fetching models for the first time.\")\n            self.pipelines = await self.get_models()\n        return self.pipelines or []\n",
    "meta": {
      "description": "Talks to Language Model Gateway and passes the OAuth ID token in the request cookies as Bearer Authorization header.",
      "manifest": {
        "title": "Language Model Gateway Pipe",
        "author": "Imran Qureshi @ b.well Connected Health (mailto:imran.qureshi@bwell.com)",
        "author_url": "https://github.com/imranq2",
        "version": "0.1.0"
      }
    },
    "is_active": true,
    "is_global": true,
    "updated_at": 1745989387,
    "created_at": 1745989309
  }
]